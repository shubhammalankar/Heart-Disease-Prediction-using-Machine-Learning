# Heart-Disease-Prediction-using-Machine-Learning

In this project the training has been done using two algorithms namely, **Support Vector Machine(SVM)** and **Random Forest**

## Dataset Characteristics
The dataset has been imported from the University of California, Irvine repository for machine learning. The dataset is multivariate with total 75 attributes. The attributes consist of categorical, numeric, binary and continuous attributes. Out of these 75 attributes, 14 major attributes are considered for this problem. The total number of instances are 303. There are 5 labels which are 0 for no disease and 1-4 for the progression of disease. There are multiple datasets from different sources but Cleveland dataset is used here as it has a smaller number of missing values hence is more accurate. It consists of 0.2% missing values and is used for classification problem.
Anomaly in Dataset:
During the analysis of the dataset it is found that the dataset is highly imbalanced. Apart from the label 0, the entries for the labels 1-4 are under sampled.  This means that there is not enough data separately for these labels to effectively predict the presence of heart disease, which would result in low accuracy and precision. For example if a dataset consists of 100 instances out of which the tuples for label 1 are 98 and for the label 2 are only 2, we say that the data for label 2 is under sampled, so even if the classifier predicts all the inputs to be true it would be 98% accurate but actually it won’t be able to classify correctly.
To deal with this problem, synthetic minority oversampling can be done by adding the instances of the under sampled labels, or the majority class data can be under sampled by removing its entries. With the kind of data available, under sampling and over sampling will not affect the accuracy of the classification much. Another way to deal with this problem is to define two labels, i.e. for positive or negative diagnosis. This balances the dataset and increases the accuracy and precision of classification.

![image](https://user-images.githubusercontent.com/32535576/229744089-0d1015c8-c4f9-4e89-b23d-245f9fad0350.png)


## Support Vector Machine
Support Vector Machines are based on the idea of graphically representing the data points in space and deriving a geometrical shape which can segregate data points to classify data. SVM provides high performing algorithm without needing much optimization. It is one of the oldest and widely used machine learning algorithm used for classification.
It is a supervised machine learning algorithm, which can be used for mainly classification and often for regression purposes.
An SVM algorithm is based on finding a hyperplane, a geometrical figure which is defined in such a way that it can differentiate the data points in space. An illustration of SVM is given below.

![image](https://user-images.githubusercontent.com/32535576/229743347-07d8cc99-8662-4484-a10f-2859d09e5581.png)

Support vectors are the data points in space nearest to the hyperplane, the points of a data set that would determine the position of the hyperplane. So support vectors can be considered most critical elements of a support vector machine.

## Random Forest
A Random Forest is a classifier consisting of collection of tree-structured classifiers where independent random vectors are distributed identically and each tree cast a unit vote for the most popular class at input x.A random vector is generated which is independent of the past random vectors with same distribution and a tree is generated by using the training test.For random forests, an upper bound is derived to obtain the generalization error in terms of two parameters that are given below:
•	The accuracy of individual classifiers
•	The dependency between the individual classifiers
•	The generalization of error for random forest includes two segments. These segments are defined below:
•	The strength of the individual classifiers in the forest.
•	The correlation between them in terms of raw margin function
How random forests work ?
•	A different subset of the training data are selected (~2/3), with replacement, to train each tree
•	Remaining training data (OOB-out of box) are used toestimate error and variable importance
•	Class assignment is made by the number of votes from all of the trees and for regression the average of the results is used 
Advantages of random forests :
•	No need for pruning trees
•	Accuracy and variable importance generated automatically
•	Overfiting is not a problem
•	Not very sensitive to outliers in training data
•	Easy to set parameters

## Graphical User Interface:-
The gui takes the data of the attributes which helps in predicting the heart disease. The gui is implemented using both the algorithms which enables the analyzer to calculate the difference between both the predictions.


## Result
The accuracy obtain for the two mechanisms totally depends on the data set used, also the prediction depends on two types i)detecting the presence of heart attack ii) how much severe is the condition of the heart attack.
The accuracy for detecting the presence of heart attack is for svm is 84.61% and for random forest is 86.81% and how much severe is the heart attack is for svm is   60.44% and for random forest is 59.34%.
